<h2 align="center">
  <span><img src="assets/leo.svg" width="4%" style="transform: translate(0,9px)"></span><b>An Embodied Generalist Agent in 3D World</b>
</h2>

<div align="center" margin-bottom="6em">
<a target="_blank" href="https://huangjy-pku.github.io/">Jiangyong Huang<sup>✶</sup></a>,
<a target="_blank" href="https://silongyong.github.io/">Silong Yong<sup>✶</sup></a>,
<a target="_blank" href="https://jeasinema.github.io/">Xiaojian Ma<sup>✶</sup></a>,
<a target="_blank" href="https://github.com/Germany321">Xiongkun Linghu<sup>✶</sup></a>,
<a target="_blank" href="https://xiaoyao-li.github.io/">Puhao Li</a>,
<br/>
<a target="_blank" href="https://github.com/jetpackfirstme">Yan Wang</a>,
<a target="_blank" href="https://liqing-ustc.github.io/">Qing Li</a>,
<a target="_blank" href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
<a target="_blank" href="https://buzz-beater.github.io/">Baoxiong Jia</a>,
<a target="_blank" href="https://siyuanhuang.com/">Siyuan Huang</a>

</div>
&nbsp;

<div align="center">
    <a href="https://arxiv.org/abs/2311.12871" target="_blank">
    <img src="https://img.shields.io/badge/Paper-arXiv-deepgreen" alt="Paper arXiv"></a>
    <a href="https://embodied-generalist.github.io" target="_blank">
    <img src="https://img.shields.io/badge/Page-LEO-9cf" alt="Project Page"></a>
    <a href="https://youtu.be/mlnjz4eSjB4?si=NN9z7TpkTPgBAzBw" target="_blank">
    <img src="https://img.shields.io/badge/Video-YouTube-9966ff" alt="Video"></a>
    <a href="placeholder" target="_blank">
    <img src="https://img.shields.io/badge/Data-LEO-blue" alt="Data"></a>
    <a href="placeholder" target="_blank">
    <img src="https://img.shields.io/badge/Model-LEO-darkorange" alt="Model"></a>
</div>
&nbsp;

<div align="left">
<img src="assets/teaser.png" width="99%" alt="LEO Teaser">
</div>

We introduce LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world, which is trained with shared LLM-based model architectures, objectives, and weights in two stages: (i) 3D vision-language alignment and (ii) 3D vision-language-action instruction tuning.\
To facilitate the training, we meticulously collect extensive datasets comprising 3D object-level and scene-level multi-modal tasks.\
LEO is capable of handling a wide spectrum of tasks, including 3D captioning, embodied reasoning, dialogue, task planning, embodied navigation, and robotic manipulation.


## BibTex
```bibtex
@article{huang2023embodied,
  title={An Embodied Generalist Agent in 3D World},
  author={Huang, Jiangyong and Yong, Silong and Ma, Xiaojian and Linghu, Xiongkun and Li, Puhao and Wang, Yan and Li, Qing and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
  journal={arXiv preprint arXiv:2311.12871},
  year={2023}
}
```
